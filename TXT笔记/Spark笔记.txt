1.Spark概述
	1.Spark简介
		Spark是一种基于内存的快速、通用、可扩展的大数据分析计算引擎
	2.Spark内置模块
		- Spark Core
			#实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块#
			#Spark Core中还包含了对弹性分布式数据集(Resilient Distributed DataSet，简称RDD)的API定义
		- Spark SQL
			#是Spark用来操作结构化数据的程序包。通过Spark SQL，我们可以使用 SQL或者Apache Hive版本的HQL来查询数据#
			#Spark SQL支持多种数据源，比如Hive表、Parquet以及JSON等
		- Spark Streaming
			#是Spark提供的对实时数据进行流式计算的组件。提供了用来操作数据流的API，并且与Spark Core中的 RDD API高度对应
		- Spark MLlib
			#提供常见的机器学习功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据导入等额外的支持功能
		- Spark GraphX
			#主要用于图形并行计算和图挖掘系统的组件
		- 集群管理器
			#Spark设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算。为了实现这样的要求，同时获得最大灵活性，Spark支持在各种集群管理器(Cluster Manager)上运行，包括Hadoop YARN、Apache Mesos，以及Spark自带的一个简易调度器，叫作独立调度器
	3.Spark特点
		1. 快:
			与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上
			Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的
		2. 易用:
			Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用
			而且Spark支持交互式的Python和Scala的Shell，可以非常方便地在这些Shell中使用Spark集群来验证解决问题的方法
		3. 通用:
			Spark提供了统一的解决方案。Spark可以用于，交互式查询(Spark SQL)、实时流处理(Spark Streaming)、机器学习(Spark MLlib)和图计算(GraphX)
			这些不同类型的处理都可以在同一个应用中无缝使用。减少了开发和维护的人力成本和部署平台的物力成本
		4. 兼容性:
			Spark可以非常方便地与其他的开源产品进行融合
			比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase等
			这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力

2.Spark运行模式
	部署Spark集群大体上分为两种模式:单机模式与集群模式
	大多数分布式框架都支持单机模式，方便开发者调试框架的运行环境。但是在生产环境中，并不会使用单机模式
	下面详细列举了Spark目前支持的部署模式:
		1. Local模式:在本地部署单个Spark服务
		2. Standalone模式:Spark自带的任务调度模式(国内不常用)
		3. YARN模式:Spark使用Hadoop的YARN组件进行资源与任务调度(国内最常用)
		4. Mesos模式:Spark使用Mesos平台进行资源与任务的调度(国内很少用)
	1.Local模式
		1. 安装解压即可
		2. 官方求PI案例
		bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[2] ./examples/jars/spark-examples_2.12-3.3.1.jar 10
			--class:表示要执行程序的主类
			--master local[2]
				1. local:没有指定线程数，则所有计算都运行在一个线程当中，没有任何并行计算
				2. local[K]:指定使用K个Core来运行计算，比如local[2]就是运行2个Core来执行
				3. local[*]:默认模式。自动帮你按照CPU最多核来设置线程数。比如CPU有8核，Spark帮你自动设置8个线程计算
			spark-examples_2.12-3.3.1.jar:要运行的程序
			10:要运行程序的输入参数(计算圆周率π的次数，计算次数越多，准确率越高)
	2.Yarn模式
		1.安装解压，重命名为spark-yarn
		2.修改spark-env.sh
			添加YARN_CONF_DIR配置，保证后续运行任务的路径都变成集群路径
			cd /opt/module/spark-yarn/conf
			mv spark-env.sh.template spark-env.sh
			vim spark-env.sh
				YARN_CONF_DIR=/opt/module/hadoop-3.3.4/etc/hadoop
		3.配置历史服务
			针对Yarn模式，再次配置一下历史服务器
			1. 修改spark-default.conf.template名称
				mv spark-defaults.conf.template spark-defaults.conf
			2. 修改spark-default.conf文件，配置日志存储路径
				vim spark-defaults.conf
					spark.eventLog.enabled           true
					spark.eventLog.dir               hdfs://hadoop102:8020/directory
			3. 修改spark-env.sh文件，添加如下配置:
				vim spark-env.sh
					export SPARK_HISTORY_OPTS="
					-Dspark.history.ui.port=18080
					-Dspark.history.fs.logDirectory=hdfs://hadoop102:8020/directory
					-Dspark.history.retainedApplications=30"
					#参数1:WEBUI访问的端口号为18080
					#参数2:指定历史服务器日志存储路径(读)
					#参数3:指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数
			4. 配置查看历史日志
				为了能从Yarn上关联到Spark历史服务器，需要配置spark历史服务器关联路径
				目的是点击yarn(8088)上spark任务的history按钮，进入的是spark历史服务器(18080)，而不再是yarn历史服务器(19888)
				1. 修改配置文件/opt/module/spark-yarn/conf/spark-defaults.conf，添加如下内容:
					spark.yarn.historyServer.address=hadoop102:18080
					spark.history.ui.port=18080
				2. 重启Spark历史服务
					sbin/stop-history-server.sh
					sbin/start-history-server.sh
		4.运行流程
			Spark有yarn-client和yarn-cluster两种模式，主要区别在于:Driver程序的运行节点
			yarn-client:Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出
			yarn-cluster:Driver程序运行在由ResourceManager启动的APPMaster，适用于生产环境
			1. 客户端模式(默认)
				bin/spark-submit \
				--class org.apache.spark.examples.SparkPi \
				--master yarn \
				--deploy-mode client \
				./examples/jars/spark-examples_2.12-3.3.1.jar \
				10
			2. 集群模式
				bin/spark-submit \
				--class org.apache.spark.examples.SparkPi \
				--master yarn \
				--deploy-mode cluster \
				./examples/jars/spark-examples_2.12-3.3.1.jar \
				10

3.RDD
	1.RDD概述
		1. RDD(Resilient Distributed Dataset)
			弹性分布式数据集，是Spark中最基本的数据抽象，Spark中最基本的数据处理模型(业务模型)
			代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合
			RDD封装的就是分布式计算功能:数据传输、数据执行
			RDD会提供很多方法，并且每一个方法不会很复杂，可以组合多个RDD方法来实现复杂逻辑
		2. RDD五大特性
			1. 一组分区(Partition)，即是数据集的基本组成单位，标记数据是哪个分区的
			2. 一个计算每个分区的函数
			3. RDD之间的依赖关系
			4. 一个Partitioner，即RDD的分片函数，控制分区的数据流向(键值对)
			5. 一个列表，存储存取每个Partition的优先位置(preferred location)，如果节点和分区个数不对应优先把分区设置在哪个节点上。移动数据不如移动计算，除非资源不够
	2.RDD的创建
		在Spark中创建RDD的创建方式可以分为三种:从集合中创建RDD、从外部存储创建RDD、从其他RDD创建
		在pom文件中添加spark-core的依赖:
			<dependencies>
				<dependency>
					<groupId>org.apache.spark</groupId>
					<artifactId>spark-core_2.12</artifactId>
					<version>3.3.1</version>
				</dependency>
			</dependencies>
		1.从集合中创建RDD:parallelize
			public class Test01_List {
				public static void main(String[] args) {
					# 1.创建配置对象
					SparkConf conf = new SparkConf().setMaster("local[*]").setAppName("sparkCore");
					# 2.创建SparkContext
					JavaSparkContext jsc = new JavaSparkContext(conf);
					# 3.编写代码
					JavaRDD<String> rdd = jsc.parallelize(Arrays.asList("hello", "spark"));
					rdd.collect().forEach(System.out::println);
					rdd.saveAsTextFile("output");
					# 4.关闭jsc
					jsc.stop();
				}
			}
		2.从外部存储系统的数据集创建:textFile
			由外部存储系统的数据集创建RDD包括:本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、HBase等
			JavaRDD<String> rdd = jsc.textFile("input");
	3.分区规则
		1.从集合创建RDD
			1. 默认分区数为核数
				conf.set("spark.default.parallelism", "6");
					#手动设置默认核数
			2. 手动设置分区数
				JavaRDD<String> rdd = jsc.parallelize(names, 2);
					#分区数设置为2
			3. 数据分配方法
				利用整数除机制，左闭右开
				0号分区: (0 * 总字节数 / 分区数) 到 (1 * 总字节数 / 分区数)
				1号分区: (1 * 总字节数 / 分区数) 到 (2 * 总字节数 / 分区数)
				例如:
				0: start 0*5/2  end 1*5/2 => (0,2)
				1: start 1*5/2  end 2*5/2 => (2,5)
		2.从文件创建RDD
			1. 默认分区数为 核数和2的最小值
			2. 手动设置最小分区数
				JavaRDD<String> rdd = jsc.textFile("data/word.txt", 3)
					#最小分区数设置为3
			3. 分区流程
				1. 根据文件的总长度(回车，换行符也计算在内)totalSize 和 切片数numSplits 计算出 平均长度goalSize
					goalSize = totalSize / numSplits
				2. 获取块大小 blockSize = 128M
				3. 计算切分大小 splitSize = Math.max(minSize, Math.min(goalSize, blockSize));
					goalSize 和 blockSize 比较，谁小拿谁进行切分
				4. 使用splitSize按照1.1倍原则切分整个文件，如果最后剩余数据大小大于 0.1*splitSize，再加一个分区
				例如:
				totalSize = 10，numSplits = 3
				goalSize = 10 / 3 = 3(byte) 表示每个分区存储3字节的数据
				分区数 = totalSize/ goalSize = 10 /3 => 3,3,4
				4byte大于3byte的1.1倍,符合Hadoop切片1.1倍的策略,因此会多创建一个分区,即一共有4个分区
			4. 数据分配方法
				Spark采用Hadoop的方式读取，所以按行读取
				读取数据时会依靠换行符进行读取，读取到回车符会忽略
				如果切分的位置位于一行的中间，会在当前分区读完一整行数据
				数据读取位置计算是以偏移量为单位来进行计算的，从0开始
				例如:
				0: [0,3](如果没读满3个，最多会读到偏移量3的位置，若偏移量3不在行尾，读完此行)
				1: [3,6]
				2: [6,9]
				3: [9,10]
	4.Transformation 转换算子
		算子:
			问题: 初始状态 --operate-> 中间状态 --operate-> 完成状态
			operate即为算子
		转换:
			transform，将A变成B
			将一个RDD(旧)变成另外一个RDD(新)
			转换的目的: 组合功能
			装饰者设计模式
		转换算子只是用于组合功能，不会马上执行
		返回类型为RDD
		1. RDD方法在调用时，默认分区数量和前面RDD的分区数量相同
		2. RDD方法在调用时，默认情况下，数据处理完后，所在分区不变
		3. RDD方法在调用时，分区内有序，分区间无序
		4. 元组
			Tuple2<> tuple = new Tuple2<>(,)
			tuple._1
			tuple._2
		1.单值类型
			- map() 映射
				# rdd.map(num -> num + 1)#
				# rdd.mapToPair(num -> new Tuple2(num, 1))
			- flatMap() 扁平化
				# rdd.flatMap(list -> list.iterator())#
				# 将所有集合中元素取出放入一个大的集合中#
				# 需返回可迭代对象
			- filter() 过滤
				# rdd.filter(num -> num > 0)
			- groupBy() 分组
				# rdd.groupBy(num -> num % 2)#
				# 对数据添加分组标记，按标记分组，将同一组的数据放入一个迭代器#
				# shuffle:#
				# 	将不同的分区数据进行打乱重组的过程，称之为shuffle#
				# 	分组落盘#
				# 	同组数据放入同一分区#
				# shuffle存在的问题:#
				# 	分区数量可能不合理(分组数量与分区数量不匹配)#
				# 	不允许出现环状功能调用#
				# 	shuffle操作会影响Spark的计算性能#
				# shuffle操作一般都会提供改变分区的功能#
				# 底层调用groupByKey()
			- distinct() 全局去重
				# rdd.distinct()#
				# 底层存在shuffle操作#
				# 底层使用了reduceByKey()
			- sortBy() 排序
				# rdd.sortBy(num -> num, true, 2)#
				# 参数1: 排序的规则#
				# 	给每一个数据增加排序标记，根据标记大小对数据排序#
				# 参数2: 升序或降序，true为升序#
				# 参数3: 设定分区数量#
				# 底层调用sortByKey()
			- coalesce() 合并分区
				# rdd.coalesce(2, false)#
				# 默认没有shuffle操作，数据不会打乱重组#
				# 可能存在数据倾斜，若倾斜可以使用shuffle操作
			- repartition() 重新分区
				# rdd.repartition(3)#
				# 底层调用coalesce()
		2.键值类型
			- mapValues() 对value操作
				# rdd.mapValues(val -> val * 2)
			- groupByKey() 按key对value分组
				# rdd.groupByKey()
			- reduceByKey() 按key对value两两聚合
				# rdd.reduceByKey((val1, val2) -> val1 + val2)#
				# reduceByKey会进行两次计算:分区内计算+分区间计算#
				# reduceByKey会在shuffle之前进行预聚合
			- sortByKey() 按key排序
				# rdd.sortByKey()
	5.Action 行动算子
		行动算子调用后就会执行功能
		返回类型为集合
		执行时就会产生一个Job
		代码执行位置:
			RDD的转换算子在调用时，逻辑并不会执行，会在逻辑发送到Executor端才执行
				RDD算子的内部代码在Executor端执行
				RDD算子的外部代码在Driver端执行
				当算子的内部代码使用了外部代码的数据或对象，那么就需要从Driver端将对象或数据拉取到Executor端
		- collect()
			# rdd.collect()#
			# 会将结果从EXecutor按照分区顺序依次拉取回Driver#
			# 可能存在Driver中资源不够的问题，故生产环境中不推荐使用
		- first() 首个
			# rdd.first()
		- take() 前几个
			# rdd.take(3)
		- takeOrdered()
			# rdd.takeOrdered(3)#
			# 先排序，再取前几个
		- count() 计数
			# rdd.count()
		- countByKey() 对key计数
			# rdd.countByKey()
		- countByValue() 对单值计数
			# rdd.countByValue()
		- save()
			# rdd.saveAsTextFile("output")#
			# rdd.saveAsObjectFile("output")
		- foreach() 遍历
			# rdd.foreach()#
			# 分布式循环遍历，以分区为单位循环，可能导致结果无序
		- foreachPartition() 对每个分区内遍历
			# rdd.foreachPartition()