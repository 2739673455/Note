1.Spark概述
	1.Spark简介
		Spark是一种基于内存的快速、通用、可扩展的大数据分析计算引擎
	2.Spark内置模块
		- Spark Core
			#实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块#
			#Spark Core中还包含了对弹性分布式数据集(Resilient Distributed DataSet，简称RDD)的API定义
		- Spark SQL
			#是Spark用来操作结构化数据的程序包。通过Spark SQL，我们可以使用 SQL或者Apache Hive版本的HQL来查询数据#
			#Spark SQL支持多种数据源，比如Hive表、Parquet以及JSON等
		- Spark Streaming
			#是Spark提供的对实时数据进行流式计算的组件。提供了用来操作数据流的API，并且与Spark Core中的 RDD API高度对应
		- Spark MLlib
			#提供常见的机器学习功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据导入等额外的支持功能
		- Spark GraphX
			#主要用于图形并行计算和图挖掘系统的组件
		- 集群管理器
			#Spark设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算。为了实现这样的要求，同时获得最大灵活性，Spark支持在各种集群管理器(Cluster Manager)上运行，包括Hadoop YARN、Apache Mesos，以及Spark自带的一个简易调度器，叫作独立调度器
	3.Spark特点
		1. 快:
			与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上
			Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的
		2. 易用:
			Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用
			而且Spark支持交互式的Python和Scala的Shell，可以非常方便地在这些Shell中使用Spark集群来验证解决问题的方法
		3. 通用:
			Spark提供了统一的解决方案。Spark可以用于，交互式查询(Spark SQL)、实时流处理(Spark Streaming)、机器学习(Spark MLlib)和图计算(GraphX)
			这些不同类型的处理都可以在同一个应用中无缝使用。减少了开发和维护的人力成本和部署平台的物力成本
		4. 兼容性:
			Spark可以非常方便地与其他的开源产品进行融合
			比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase等
			这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力

2.Spark运行模式
	部署Spark集群大体上分为两种模式:单机模式与集群模式
	大多数分布式框架都支持单机模式，方便开发者调试框架的运行环境。但是在生产环境中，并不会使用单机模式
	下面详细列举了Spark目前支持的部署模式:
		1. Local模式:在本地部署单个Spark服务
		2. Standalone模式:Spark自带的任务调度模式(国内不常用)
		3. YARN模式:Spark使用Hadoop的YARN组件进行资源与任务调度(国内最常用)
		4. Mesos模式:Spark使用Mesos平台进行资源与任务的调度(国内很少用)
	1.Local模式
		1. 安装解压即可
		2. 官方求PI案例
		bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[2] ./examples/jars/spark-examples_2.12-3.3.1.jar 10
			--class:表示要执行程序的主类
			--master local[2]
				1. local:没有指定线程数，则所有计算都运行在一个线程当中，没有任何并行计算
				2. local[K]:指定使用K个Core来运行计算，比如local[2]就是运行2个Core来执行
				3. local[*]:默认模式。自动帮你按照CPU最多核来设置线程数。比如CPU有8核，Spark帮你自动设置8个线程计算
			spark-examples_2.12-3.3.1.jar:要运行的程序
			10:要运行程序的输入参数(计算圆周率π的次数，计算次数越多，准确率越高)
	2.Yarn模式
		1.安装解压，重命名为spark-yarn
		2.修改spark-env.sh
			添加YARN_CONF_DIR配置，保证后续运行任务的路径都变成集群路径
			cd /opt/module/spark-yarn/conf
			mv spark-env.sh.template spark-env.sh
			vim spark-env.sh
				YARN_CONF_DIR=/opt/module/hadoop-3.3.4/etc/hadoop
		3.配置历史服务
			针对Yarn模式，再次配置一下历史服务器
			1. 修改spark-default.conf.template名称
				mv spark-defaults.conf.template spark-defaults.conf
			2. 修改spark-default.conf文件，配置日志存储路径
				vim spark-defaults.conf
					spark.eventLog.enabled           true
					spark.eventLog.dir               hdfs://hadoop102:8020/directory
			3. 修改spark-env.sh文件，添加如下配置:
				vim spark-env.sh
					export SPARK_HISTORY_OPTS="
					-Dspark.history.ui.port=18080
					-Dspark.history.fs.logDirectory=hdfs://hadoop102:8020/directory
					-Dspark.history.retainedApplications=30"
					#参数1:WEBUI访问的端口号为18080
					#参数2:指定历史服务器日志存储路径(读)
					#参数3:指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数
			4. 配置查看历史日志
				为了能从Yarn上关联到Spark历史服务器，需要配置spark历史服务器关联路径
				目的是点击yarn(8088)上spark任务的history按钮，进入的是spark历史服务器(18080)，而不再是yarn历史服务器(19888)
				1. 修改配置文件/opt/module/spark-yarn/conf/spark-defaults.conf，添加如下内容:
					spark.yarn.historyServer.address=hadoop102:18080
					spark.history.ui.port=18080
				2. 重启Spark历史服务
					sbin/stop-history-server.sh
					sbin/start-history-server.sh
		4.运行流程
			Spark有yarn-client和yarn-cluster两种模式，主要区别在于:Driver程序的运行节点
			yarn-client:Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出
			yarn-cluster:Driver程序运行在由ResourceManager启动的APPMaster，适用于生产环境
			1. 客户端模式(默认)
				bin/spark-submit \
				--class org.apache.spark.examples.SparkPi \
				--master yarn \
				--deploy-mode client \
				./examples/jars/spark-examples_2.12-3.3.1.jar \
				10
			2. 集群模式
				bin/spark-submit \
				--class org.apache.spark.examples.SparkPi \
				--master yarn \
				--deploy-mode cluster \
				./examples/jars/spark-examples_2.12-3.3.1.jar \
				10

3.RDD
	1.RDD概述
		1. RDD(Resilient Distributed Dataset)
			弹性分布式数据集，是Spark中最基本的数据抽象，Spark中最基本的数据处理模型(业务模型)
			代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合
			RDD封装的就是分布式计算功能:数据传输、数据执行
			RDD会提供很多方法，并且每一个方法不会很复杂，可以组合多个RDD方法来实现复杂逻辑
		2. RDD五大特性
			1. 一组分区(Partition)，即是数据集的基本组成单位，标记数据是哪个分区的
			2. 一个计算每个分区的函数
			3. RDD之间的依赖关系
			4. 一个Partitioner，即RDD的分片函数，控制分区的数据流向(键值对)
			5. 一个列表，存储存取每个Partition的优先位置(preferred location)，如果节点和分区个数不对应优先把分区设置在哪个节点上。移动数据不如移动计算，除非资源不够
	2.RDD的创建
		在Spark中创建RDD的创建方式可以分为三种:从集合中创建RDD、从外部存储创建RDD、从其他RDD创建
		在pom文件中添加spark-core的依赖:
			<dependencies>
				<dependency>
					<groupId>org.apache.spark</groupId>
					<artifactId>spark-core_2.12</artifactId>
					<version>3.3.1</version>
				</dependency>
			</dependencies>
		1.从集合中创建RDD:parallelize
			public class Test01_List {
				public static void main(String[] args) {
					# 1.创建配置对象
					SparkConf conf = new SparkConf().setMaster("local[*]").setAppName("sparkCore");
					# 2.创建SparkContext
					JavaSparkContext jsc = new JavaSparkContext(conf);
					# 3.编写代码
					JavaRDD<String> rdd = jsc.parallelize(Arrays.asList("hello", "spark"));
					rdd.collect().forEach(System.out::println);
					rdd.saveAsTextFile("output");
					# 4.关闭jsc
					jsc.stop();
				}
			}
		2.从外部存储系统的数据集创建:textFile
			由外部存储系统的数据集创建RDD包括:本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、HBase等
			JavaRDD<String> rdd = jsc.textFile("input");
	3.分区规则
		1.从集合创建RDD
			1. 默认分区数为核数
				conf.set("spark.default.parallelism", "6");
					#手动设置默认核数
			2. 手动设置分区数
				JavaRDD<String> rdd = jsc.parallelize(names, 2);
					#分区数设置为2
			3. 数据分配方法
				利用整数除机制，左闭右开
				0号分区: (0 * 总字节数 / 分区数) 到 (1 * 总字节数 / 分区数)
				1号分区: (1 * 总字节数 / 分区数) 到 (2 * 总字节数 / 分区数)
				例如:
				0: start 0*5/2  end 1*5/2 => (0,2)
				1: start 1*5/2  end 2*5/2 => (2,5)
		2.从文件创建RDD
			1. 默认分区数为 核数和2的最小值
			2. 手动设置最小分区数
				JavaRDD<String> rdd = jsc.textFile("data/word.txt", 3)
					#最小分区数设置为3
			3. 分区流程
				1. 根据文件的总长度(回车，换行符也计算在内)totalSize 和 切片数numSplits 计算出 平均长度goalSize
					goalSize = totalSize / numSplits
				2. 获取块大小 blockSize = 128M
				3. 计算切分大小 splitSize = Math.max(minSize, Math.min(goalSize, blockSize));
					goalSize 和 blockSize 比较，谁小拿谁进行切分
				4. 使用splitSize按照1.1倍原则切分整个文件，如果最后剩余数据大小大于 0.1*splitSize，再加一个分区
				例如:
				totalSize = 10，numSplits = 3
				goalSize = 10 / 3 = 3(byte) 表示每个分区存储3字节的数据
				分区数 = totalSize/ goalSize = 10 /3 => 3,3,4
				4byte大于3byte的1.1倍,符合Hadoop切片1.1倍的策略,因此会多创建一个分区,即一共有4个分区
			4. 数据分配方法
				Spark采用Hadoop的方式读取，所以按行读取
				读取数据时会依靠换行符进行读取，读取到回车符会忽略
				如果切分的位置位于一行的中间，会在当前分区读完一整行数据
				数据读取位置计算是以偏移量为单位来进行计算的，从0开始
				例如:
				0: [0,3](如果没读满3个，最多会读到偏移量3的位置，若偏移量3不在行尾，读完此行)
				1: [3,6]
				2: [6,9]
				3: [9,10]
	4.Transformation 转换算子
		装饰者设计模式
		转换算子只是用于组合功能，不会马上执行
		转换算子返回类型为RDD
		1. RDD方法在调用时，默认分区数量和前面RDD的分区数量相同
		2. RDD方法在调用时，默认情况下，数据处理完后，所在分区不变
		3. RDD方法在调用时，分区内有序，分区间无序
		4. 元组
			Tuple2<> tuple = new Tuple2<>(,)
			tuple._1
			tuple._2
		1.单值类型
			- map() 映射
				# rdd.map(num -> num + 1)#
				# rdd.mapToPair(num -> new Tuple2(num, 1))
			- flatMap() 扁平化
				# rdd.flatMap(list -> list.iterator())#
				# 将所有集合中元素取出放入一个大的集合中#
				# 需返回可迭代对象
			- filter() 过滤
				# rdd.filter(num -> num > 0)
			- groupBy() 分组
				# rdd.groupBy(num -> num % 2)#
				# 对数据添加分组标记，按标记分组，将同一组的数据放入一个迭代器#
				# shuffle:#
				# 	将不同的分区数据进行打乱重组的过程，称之为shuffle#
				# 	分组落盘#
				# 	同组数据放入同一分区#
				# shuffle存在的问题:#
				# 	分区数量可能不合理(分组数量与分区数量不匹配)#
				# 	不允许出现环状功能调用#
				# 	shuffle操作会影响Spark的计算性能#
				# shuffle操作一般都会提供改变分区的功能#
				# 底层调用groupByKey()
			- distinct() 全局去重
				# rdd.distinct()#
				# 底层存在shuffle操作#
				# 底层使用了reduceByKey()
			- sortBy() 排序
				# rdd.sortBy(num -> num, true, 2)#
				# 参数1: 排序的规则#
				# 	给每一个数据增加排序标记，根据标记大小对数据排序#
				# 参数2: 升序或降序，true为升序#
				# 参数3: 设定分区数量#
				# 底层调用sortByKey()
			- coalesce() 合并分区
				# rdd.coalesce(2, false)#
				# 默认false，没有shuffle操作，数据不会打乱重组#
				# 可能存在数据倾斜，若倾斜可以使用shuffle操作
			- repartition() 重新分区
				# rdd.repartition(3)#
				# 底层调用coalesce()，存在shuffle操作
		2.键值类型
			- mapValues() 对value操作
				# rdd.mapValues(val -> val * 2)
			- groupByKey() 按key对value分组
				# rdd.groupByKey()
			- reduceByKey() 按key对value两两聚合
				# rdd.reduceByKey((val1, val2) -> val1 + val2)#
				# reduceByKey会进行两次计算:分区内计算+分区间计算#
				# reduceByKey会在shuffle之前进行预聚合
			- sortByKey() 按key排序
				# rdd.sortByKey()
	5.Action 行动算子
		行动算子调用后就会执行功能
		行动算子返回类型为集合
		行动算子执行时就会产生一个Job
		代码执行位置:
			RDD的转换算子在调用时，逻辑并不会执行，会在逻辑发送到Executor端才执行
				RDD算子的内部代码在Executor端执行
				RDD算子的外部代码在Driver端执行
				当算子的内部代码使用了外部代码的数据或对象，那么就需要从Driver端将对象或数据拉取到Executor端
		- collect()
			# rdd.collect()#
			# 会将结果从EXecutor按照分区顺序依次拉取回Driver#
			# 可能存在Driver中资源不够的问题，故生产环境中不推荐使用
		- first() 首个
			# rdd.first()
		- take() 前几个
			# rdd.take(3)
		- takeOrdered()
			# rdd.takeOrdered(3)#
			# 先排序，再取前几个
		- count() 计数
			# rdd.count()
		- countByKey() 对key计数
			# rdd.countByKey()
		- countByValue() 对单值计数
			# rdd.countByValue()
		- save()
			# rdd.saveAsTextFile("output")#
			# rdd.saveAsObjectFile("output")
		- foreach() 遍历所有元素
			# rdd.foreach()#
			# 分布式循环遍历，以分区为单位循环，可能导致结果无序
		- foreachPartition() 遍历所有分区
			# rdd.foreachPartition()
	6.Kryo序列化框架
		当RDD在Shuffle数据的时候，简单数据类型、数组和字符串类型已经在Spark内部使用Kryo来序列化
		使用Kryo序列化:
			// 创建配置对象
			SparkConf conf = new SparkConf().setMaster("local[*]").setAppName("sparkCore")
			// 替换默认的序列化机制
			.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
			// 注册需要使用kryo序列化的自定义类
			.registerKryoClasses(new Class[]{Class.forName("com.atguigu.bean.User")});
	7.RDD依赖关系
		1.什么是依赖
			2个相邻RDD之间存在依赖关系，新的(下游)RDD依赖于旧的(上游)RDD
			连续的依赖关系称之为血缘关系
			rdd.toDebugString()  #查看依赖关系
		2.依赖的2个分类
			1. 窄依赖: OneToOneDependency(继承NarrowDependency)
				上游RDD的一个分区的数据被下游RDD的一个分区所独享(一对一，多对一)
			2. 宽依赖: ShuffleDependency(继承Dependency)
				上游RDD的一个分区的数据被下游RDD的多个分区所共享(一对多)
				底层存在shuffle操作
				具有宽依赖的transformations包括:sort、reduceByKey、groupByKey、join和调用rePartition函数的任何操作
		3.Spark中的数量
			1. Application
				代码中Spark环境对象的数量，其实就是应用程序的数量
				一般情况下，一个Java程序就是一个Application
			2. Job
				代码中，执行行动算子的时候，调用一次就会产生一个Job(new Action)
			3. Stage
				Spark中，shuffle操作会将完整的计算流程分成两个阶段，前一个阶段不执行完，后一个阶段不允许执行
				阶段的数量 = shuffle依赖的数量 + 1
			4. Task
				一个阶段中任务的数量 = 阶段中最后一个RDD的分区数量
				总任务的数量 = 总分区的数量，一般推荐设定为CPU核数的2~3倍
	8.持久化
		1.Cache 缓存
			1. 使用方法
				rdd.cache()                          #将RDD中数据放入内存
				rdd.persist(StorageLevel.DISK_ONLY)  #将RDD中数据放入磁盘
			2. 存储位置
				Cache缓存的数据通常存储在磁盘、内存等地方，可靠性低
			3. 执行时机
				并不是这两个方法被调用时立即缓存，而是触发后面的Action算子时，该RDD将会被缓存在计算节点的内存中，供后面重用
			4. 血缘关系
				缓存操作会在RDD的血缘关系中增加依赖
			5. 自动缓存
				shuffle算子不需要手动cache，会自动缓存
			6. 释放缓存
				如果使用完了缓存，可以通过unpersist()方法释放缓存
		2.CheckPoint 检查点
			cache和persist方法只对当前应用有效，一旦应用程序执行完毕，缓存和持久化的数据全部删除
			如果要跨应用使用计算的中间数据，可以采用检查点的操作
			1. 使用方法
				jsc.setCheckPointDir("path")  #设置检查点存储路径
				rdd.checkpoint()              #检查点操作
			2. 存储位置
				检查点的数据通常是存储在HDFS等容错、高可用的文件系统
				如果检查点数据存储到HDFS集群，要注意配置访问集群的用户名。否则会报访问权限异常:
					// 修改用户名称
					System.setProperty("HADOOP_USER_NAME","atguigu");
					// 需要设置路径.需要提前在HDFS集群上创建/checkpoint路径
					jsc.setCheckpointDir("hdfs://hadoop102:8020/checkpoint");
			3. 执行时机
				检查点操作并不会马上被执行，必须执行Action操作才能触发。但是检查点为了数据安全，会从血缘关系的最开始执行一遍
			4. 血缘关系
				检查点操作会切断RDD的血缘关系，重新创建新的血缘
			5. 加快读取
				检查点操作一般为了提高效率，会和Cache操作联合使用
	9.分区器
		Spark目前支持Hash分区(默认)、Range分区和用户自定义分区
		单值类型数据没有分区器
		键值类型数据才有分区器，数据的key决定数据所在分区
		1.HashPartitioner
			key.hashCode() % numPartitions
			可能出现数据倾斜
		2.RangePartitioner
			将一定范围内的数映射到某一个分区内，尽量保证每个分区中数据量均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大，但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内
			1. 从整个RDD中抽取样本数据并排序，根据分区数将数据划分出相应段数，计算出每个分区的最大key值，形成一个Array[key]类型的数组变量rangeBounds
			2. 根据每个key在rangeBounds中所处的范围，给出该key的分区号
			3. 该分区器要求RDD中的key类型必须是可以排序的
		3.自定义分区器
			class MyPartitioner extends Partitioner {
				@Override
				public int numPartitions() {
					return 2;
				}

				@Override
				public int getPartition(Object key) {
					if ((Integer) key % 2 == 0) return 0;
					else return 1;
				}
			}
	10.广播变量
		1. 定义
			广播变量: 分布式共享只读变量
		2. 为什么使用广播变量
			多个Task无法共享对象
			多个Task并行操作中使用同一个变量，但是Spark会为每个Task任务分别发送，可能出现同一数据传输多次，出现冗余
		3. 广播变量的优点
			广播变量用来高效分发较大的对象
			广播变量向所有工作节点(Executor)发送一个较大的只读值，以供一个或多个Task操作使用
		4. 使用方法
			// 创建广播变量
			Broadcast<> broadcast = jsc.broadcast(var);
			// 使用广播变量
			broadcast.value()
			例:
				JavaRDD<Integer> rdd = jsc.parallelize(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 0), 10);
				List<Integer> okIds = new ArrayList<>();
				okIds.add(4);
				okIds.add(6);
				okIds.add(8);
				okIds.add(12);
				okIds.add(25);
				Broadcast<List<Integer>> broadcast = jsc.broadcast(okIds);
				JavaRDD<Integer> filterRDD = rdd.filter(
						num -> broadcast.value().contains(num)
				);
				filterRDD.collect().forEach(System.out::println);